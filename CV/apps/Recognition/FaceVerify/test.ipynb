{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmF9dZK1dJCl"
      },
      "source": [
        "# Face detection and recognition training pipeline\n",
        "\n",
        "The following example illustrates how to fine-tune an InceptionResnetV1 model on your own dataset. This will mostly follow standard pytorch training patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HBftUSgdN-k",
        "outputId": "30aa79ea-bcfe-4652-d121-1830f0db1b3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'facenet-pytorch'...\n",
            "remote: Enumerating objects: 1267, done.\u001b[K\n",
            "remote: Total 1267 (delta 0), reused 0 (delta 0), pack-reused 1267\u001b[K\n",
            "Receiving objects: 100% (1267/1267), 22.88 MiB | 1.80 MiB/s, done.\n",
            "Resolving deltas: 100% (620/620), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/timesler/facenet-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oi-R6g5vdR3B",
        "outputId": "a8bb6414-7918-4dbc-face-1742618ccf7b"
      },
      "outputs": [],
      "source": [
        "!pip install -q facenet_pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zlq3GC6dJCu",
        "outputId": "86be41bf-ff61-4ce0-aecb-c0f3db4888ea"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/xps/anaconda3/envs/deeplearning/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: '/content/facenet-pytorch'\n",
            "/home/xps/projects/deep-learning-/CV/apps/Recognition/FaceVerify\n"
          ]
        }
      ],
      "source": [
        "from facenet_pytorch import MTCNN, InceptionResnetV1, fixed_image_standardization, training\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
        "from torch import optim\n",
        "from torch.optim.lr_scheduler import MultiStepLR\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "%cd /content/facenet-pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3n4OxP-dJCw"
      },
      "source": [
        "#### Define run parameters\n",
        "\n",
        "The dataset should follow the VGGFace2/ImageNet-style directory layout. Modify `data_dir` to the location of the dataset on wish to finetune on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "qBOElF5CdJCx"
      },
      "outputs": [],
      "source": [
        "data_dir = 'facenet-pytorch/data/test_images'\n",
        "\n",
        "batch_size = 32\n",
        "epochs = 8\n",
        "workers = 0 if os.name == 'nt' else 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = datasets.ImageFolder(\"/home/xps/projects/deep-learning-/CV/apps/Recognition/FaceVerify/deploy/database/test_images_cropped\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[0][1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_yP_K0kdJCy"
      },
      "source": [
        "#### Determine if an nvidia GPU is available"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7cWZw3wdJCy",
        "outputId": "5a79f727-ed5c-45c0-c5b6-44e86c2f9eab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on device: cpu\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print('Running on device: {}'.format(device))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKjFrxp1dJCz"
      },
      "source": [
        "#### Define MTCNN module\n",
        "\n",
        "See `help(MTCNN)` for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "Et6O9jCodJC0"
      },
      "outputs": [],
      "source": [
        "mtcnn = MTCNN(\n",
        "    image_size=160, margin=0, min_face_size=20,\n",
        "    thresholds=[0.6, 0.7, 0.7], factor=0.7, post_process=True,\n",
        "    device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [],
      "source": [
        "img = Image.open(\"/home/xps/projects/deep-learning-/CV/apps/Recognition/FaceVerify/deploy/database/test_images/tranbaotri_20200083/5.jpg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'shape'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m/home/xps/projects/deep-learning-/CV/apps/Recognition/FaceVerify/finetune_facenet.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/xps/projects/deep-learning-/CV/apps/Recognition/FaceVerify/finetune_facenet.ipynb#X51sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m mtcnn(img, save_path\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtests/1.jpg\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49mshape\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/xps/projects/deep-learning-/CV/apps/Recognition/FaceVerify/finetune_facenet.ipynb#X51sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m Image\u001b[39m.\u001b[39mopen(\u001b[39m'\u001b[39m\u001b[39mtests/1.jpg\u001b[39m\u001b[39m'\u001b[39m)\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
          ]
        }
      ],
      "source": [
        "mtcnn(img, save_path='tests/1.jpg').shape\n",
        "Image.open('tests/1.jpg')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHnnOPNmdJC2"
      },
      "source": [
        "#### Perfom MTCNN facial detection\n",
        "\n",
        "Iterate through the DataLoader object and obtain cropped faces."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IEpm8JZ1dJC3",
        "outputId": "5ef8191b-1f67-4253-baf6-bfceab99d27d",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['facenet-pytorch/data/test_images_cropped/tranbaotri_20200083/1.jpg']\n",
            "[<PIL.Image.Image image mode=RGB size=917x1920 at 0x7FDA6A3A9E80>]\n",
            "torch.Size([3, 160, 160])\n",
            "Batch 15 of 19['facenet-pytorch/data/test_images_cropped/tranbaotri_20200083/2.jpg']\n",
            "[<PIL.Image.Image image mode=RGB size=917x1920 at 0x7FDA6A3A9F70>]\n",
            "torch.Size([3, 160, 160])\n",
            "Batch 16 of 19['facenet-pytorch/data/test_images_cropped/tranbaotri_20200083/3.jpg']\n",
            "[<PIL.Image.Image image mode=RGB size=917x1920 at 0x7FDA6A536AF0>]\n",
            "torch.Size([3, 160, 160])\n",
            "Batch 17 of 19['facenet-pytorch/data/test_images_cropped/tranbaotri_20200083/4.jpg']\n",
            "[<PIL.Image.Image image mode=RGB size=917x1920 at 0x7FDA6A3A9F10>]\n",
            "torch.Size([3, 160, 160])\n",
            "Batch 18 of 19['facenet-pytorch/data/test_images_cropped/tranbaotri_20200083/5.jpg']\n",
            "[<PIL.Image.Image image mode=RGB size=917x1920 at 0x7FDA6A3A9E50>]\n",
            "torch.Size([3, 160, 160])\n",
            "Batch 19 of 19"
          ]
        }
      ],
      "source": [
        "dataset = datasets.ImageFolder(data_dir)\n",
        "dataset.samples = [\n",
        "    (p, p.replace(data_dir, data_dir + '_cropped'))\n",
        "        for p, _ in dataset.samples\n",
        "]\n",
        "        \n",
        "loader = DataLoader(\n",
        "    dataset,\n",
        "    num_workers=workers,\n",
        "    batch_size=1,\n",
        "    collate_fn=training.collate_pil\n",
        ")\n",
        "\n",
        "for i, (x, y) in enumerate(loader):\n",
        "    if y[0].split('/')[-2][-1] == '3':\n",
        "        print(y)\n",
        "        print(x)\n",
        "        img = x[0]\n",
        "        print(mtcnn(x[0], save_path='facenet-pytorch/tests' + f'/{i}.jpg').shape)\n",
        "        print('\\rBatch {} of {}'.format(i + 1, len(loader)), end='')\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([3, 1920, 917])"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "transforms.ToTensor()(img).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rD-vxyTdJC4"
      },
      "source": [
        "#### Define Inception Resnet V1 module\n",
        "\n",
        "See `help(InceptionResnetV1)` for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ihUxiKH7dJC4"
      },
      "outputs": [],
      "source": [
        "resnet = InceptionResnetV1(\n",
        "    classify=True,\n",
        "    pretrained='vggface2',\n",
        "    num_classes=len(dataset.class_to_idx)\n",
        ").to(device)\n",
        "for params in resnet.parameters():\n",
        "  params.requires_grad_(False)\n",
        "for params in resnet.logits.parameters():\n",
        "  params.requires_grad_(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enLnvB2xdJC5"
      },
      "source": [
        "#### Define optimizer, scheduler, dataset, and dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "sVALPtWsdJC5"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.Adam(resnet.parameters(), lr=0.001)\n",
        "scheduler = MultiStepLR(optimizer, [5, 10])\n",
        "\n",
        "trans = transforms.Compose([\n",
        "    np.float32,\n",
        "    transforms.ToTensor(),\n",
        "    fixed_image_standardization\n",
        "])\n",
        "dataset = datasets.ImageFolder(data_dir + '_cropped', transform=trans)\n",
        "img_inds = np.arange(len(dataset))\n",
        "np.random.shuffle(img_inds)\n",
        "train_inds = img_inds[:int(0.8 * len(img_inds))]\n",
        "val_inds = img_inds[int(0.8 * len(img_inds)):]\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    dataset,\n",
        "    num_workers=workers,\n",
        "    batch_size=32,\n",
        "    sampler=SubsetRandomSampler(train_inds)\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    dataset,\n",
        "    num_workers=workers,\n",
        "    batch_size=32,\n",
        "    sampler=SubsetRandomSampler(val_inds)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXT2GQYBdJC6"
      },
      "source": [
        "#### Define loss and evaluation functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "6Odls3KqdJC6"
      },
      "outputs": [],
      "source": [
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "metrics = {\n",
        "    'fps': training.BatchTimer(),\n",
        "    'acc': training.accuracy\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swZQ7LmtdJC7"
      },
      "source": [
        "#### Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "se7rh_1fdJC7",
        "outputId": "5c746c27-6bd2-46da-fad9-83f459bcb066"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Initial\n",
            "----------\n",
            "Valid |     1/1    | loss:    1.4733 | fps:    1.7771 | acc:    0.0000   \n",
            "\n",
            "Epoch 1/8\n",
            "----------\n",
            "Train |     1/1    | loss:    1.2617 | fps:   12.3735 | acc:    0.3000   \n",
            "Valid |     1/1    | loss:    1.1663 | fps:    3.3125 | acc:    0.0000   \n",
            "\n",
            "Epoch 2/8\n",
            "----------\n",
            "Train |     1/1    | loss:    0.8988 | fps:   10.7183 | acc:    0.8000   \n",
            "Valid |     1/1    | loss:    0.8966 | fps:    5.3609 | acc:    0.6667   \n",
            "\n",
            "Epoch 3/8\n",
            "----------\n",
            "Train |     1/1    | loss:    0.7252 | fps:    9.6096 | acc:    0.9000   \n",
            "Valid |     1/1    | loss:    0.6649 | fps:    5.0111 | acc:    1.0000   \n",
            "\n",
            "Epoch 4/8\n",
            "----------\n",
            "Train |     1/1    | loss:    0.5130 | fps:    9.1453 | acc:    1.0000   \n",
            "Valid |     1/1    | loss:    0.4826 | fps:    5.1708 | acc:    1.0000   \n",
            "\n",
            "Epoch 5/8\n",
            "----------\n",
            "Train |     1/1    | loss:    0.3826 | fps:   12.7967 | acc:    1.0000   \n",
            "Valid |     1/1    | loss:    0.3448 | fps:    7.6628 | acc:    1.0000   \n",
            "\n",
            "Epoch 6/8\n",
            "----------\n",
            "Train |     1/1    | loss:    0.2260 | fps:   14.6654 | acc:    1.0000   \n",
            "Valid |     1/1    | loss:    0.3228 | fps:    9.9957 | acc:    1.0000   \n",
            "\n",
            "Epoch 7/8\n",
            "----------\n",
            "Train |     1/1    | loss:    0.2430 | fps:   19.0564 | acc:    1.0000   \n",
            "Valid |     1/1    | loss:    0.3019 | fps:    9.4973 | acc:    1.0000   \n",
            "\n",
            "Epoch 8/8\n",
            "----------\n",
            "Train |     1/1    | loss:    0.2413 | fps:   16.4899 | acc:    1.0000   \n",
            "Valid |     1/1    | loss:    0.2846 | fps:    9.4635 | acc:    1.0000   \n"
          ]
        }
      ],
      "source": [
        "writer = SummaryWriter()\n",
        "writer.iteration, writer.interval = 0, 10\n",
        "\n",
        "print('\\n\\nInitial')\n",
        "print('-' * 10)\n",
        "resnet.eval()\n",
        "training.pass_epoch(\n",
        "    resnet, loss_fn, val_loader,\n",
        "    batch_metrics=metrics, show_running=True, device=device,\n",
        "    writer=writer\n",
        ")\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print('\\nEpoch {}/{}'.format(epoch + 1, epochs))\n",
        "    print('-' * 10)\n",
        "\n",
        "    resnet.train()\n",
        "    training.pass_epoch(\n",
        "        resnet, loss_fn, train_loader, optimizer, scheduler,\n",
        "        batch_metrics=metrics, show_running=True, device=device,\n",
        "        writer=writer\n",
        "    )\n",
        "\n",
        "    resnet.eval()\n",
        "    training.pass_epoch(\n",
        "        resnet, loss_fn, val_loader,\n",
        "        batch_metrics=metrics, show_running=True, device=device,\n",
        "        writer=writer\n",
        "    )\n",
        "\n",
        "writer.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVsh5wKgvEc7"
      },
      "source": [
        "#test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "kuaE99DvvUY9"
      },
      "outputs": [],
      "source": [
        "!wget https://vcdn-ngoisao.vnecdn.net/2021/10/30/settopjolie-1635526336-6968-1635526893_1200x0.jpg -q -O test1.jpg "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "_FMVE733viU5"
      },
      "outputs": [],
      "source": [
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "x2J8vVM-vXzq"
      },
      "outputs": [],
      "source": [
        "test = mtcnn(Image.open(\"/content/facenet-pytorch/data/test_images/tranbaotri/6.jpg\"), save_path='test1_mt.jpg').unsqueeze(0)\n",
        "test = torch.concat([test, test], dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exAZvedewhbm"
      },
      "outputs": [],
      "source": [
        "resnet.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1MkwWw0vsF7",
        "outputId": "41e66b8c-bef2-4dd2-c6cf-accd4aae665f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(3)"
            ]
          },
          "execution_count": 143,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.argmax(resnet(test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ph83oFfiEAgL"
      },
      "source": [
        "#Saving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "id": "unggVgMwD9F7"
      },
      "outputs": [],
      "source": [
        "!mkdir /content/facenet-pytorch/checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "id": "wWadmbF8Eg1D"
      },
      "outputs": [],
      "source": [
        "ckp_dir = \"/content/facenet-pytorch/checkpoints\"\n",
        "torch.save(resnet.state_dict(), ckp_dir + 'model.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqPRV4bR1JBJ"
      },
      "source": [
        "#New person"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "e--SIOMav2Sg"
      },
      "outputs": [],
      "source": [
        "!mkdir /content/facenet-pytorch/data/test_images/tranbaotri"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGf9w1xTDFEw"
      },
      "source": [
        "#....\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjuordGuDP0s",
        "outputId": "e5336030-5a56-44bc-cffa-e2e33c36a05b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'MiAI_FaceRecog_2'...\n",
            "remote: Enumerating objects: 129, done.\u001b[K\n",
            "remote: Counting objects: 100% (15/15), done.\u001b[K\n",
            "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
            "remote: Total 129 (delta 3), reused 9 (delta 0), pack-reused 114\u001b[K\n",
            "Receiving objects: 100% (129/129), 2.70 MiB | 6.35 MiB/s, done.\n",
            "Resolving deltas: 100% (35/35), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/thangnch/MiAI_FaceRecog_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v535uUibDdGI"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "from flask import Flask\n",
        "from flask import render_template , request\n",
        "from flask_cors import CORS, cross_origin\n",
        "import torchvision.transforms as tf\n",
        "import argparse\n",
        "import os\n",
        "import sys\n",
        "import math\n",
        "import pickle\n",
        "import align.detect_face\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "import collections\n",
        "from sklearn.svm import SVC\n",
        "import base64\n",
        "\n",
        "FACENET_MODEL_PATH = '/content/facenet-pytorch/checkpoints' + 'model.pt'\n",
        "mtcnn = MTCNN(keep_all=True, post_process=False)\n",
        "#get from config\n",
        "num_classes = 4\n",
        "class_name = ['jolie', 'john', 'badd', 'tri']\n",
        "class_dict = dict([(class_name[i], num_classes[i]) for i in range(num_classes)])\n",
        "\n",
        "def get_face(image):\n",
        "  mtcnn = MTCNN(keep_all=True, post_process=False)\n",
        "  face = mtcnn(image)[0]\n",
        "  image = transforms.Resize((160, 160))((transforms.ToTensor()((Image.fromarray(face.permute(1, 2, 0).numpy().astype(np.uint8))))))\n",
        "  image = Image.fromarray(face.permute(1, 2, 0).numpy().astype(np.uint8))\n",
        "  return image\n",
        "\n",
        "\n",
        "\n",
        "# Load the model\n",
        "print('Loading feature extraction model')\n",
        "facenet = InceptionResnetV1(\n",
        "    classify=True,\n",
        "    pretrained='vggface2',\n",
        "    num_classes=num_classes\n",
        ").to(device)\n",
        "\n",
        "facenet.load_state_dict(torch.load(FACENET_MODEL_PATH))\n",
        "facenet.eval()\n",
        "\n",
        "# Get input and output tensors by POST\n",
        "\n",
        "\n",
        "\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "\n",
        "\n",
        "\n",
        "@app.route('/')\n",
        "@cross_origin()\n",
        "def index():\n",
        "    return \"OK!\";\n",
        "\n",
        "@app.route('/recog', methods=['POST'])\n",
        "@cross_origin()\n",
        "def upload_img_file():\n",
        "    if request.method == 'POST':\n",
        "        name = \"Unknown\"\n",
        "        og_image = request.form.get('image')\n",
        "        image = get_face(og_image)\n",
        "        image = tf.ToTensor()(image)\n",
        "        logits = facenet(image)\n",
        "        name = class_dict[torch.argmax(logits).item()]\n",
        "\n",
        "\n",
        "        \n",
        "\n",
        "        return name;\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(debug=True, host='0.0.0.0',port='8000')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pz8ZqI3pGgfm"
      },
      "outputs": [],
      "source": [
        "\n",
        "get_face()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('deeplearning')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "73fdcbcaa6b22d852c0f9bd9783ab6b1b1c25c52a0a8da76beae07513436cb85"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
